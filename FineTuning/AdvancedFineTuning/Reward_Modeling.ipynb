{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Modeling\n",
    "Imagine that you're working as a machine learning engineer for a large technology company that wants to integrate advanced language models into its suite of AI-powered products. Your task is to evaluate and select the best large language model (LLM) that can understand and follow complex instructions, improve the quality of automated customer service, and generate high-quality responses.\n",
    "\n",
    "However, simply choosing a powerful LLM isn't enough. To truly excel in these tasks, the model should be fine-tuned to align with specific goals and criteria. This is where reward models come into the picture. By training a reward model, you can guide the LLM to prioritize certain behaviors, ensuring it generates responses that not only meet technical standards but also align with the company's values and objectives.\n",
    "\n",
    "In this hands-on lab, you dive into the process of creating and training a reward model by using the transformer reinforcement learner (trl) library from Hugging Face. You learn how to set up the environment, define the rewards that shape the model's behavior, and fine-tune the LLM to perform with precision. This project equips you with the skills to implement reward models in real-world applications, enhancing the effectiveness and quality of AI-powered products.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    </ul>\n",
    "    <li><a href=\"#Setup\">Setup</a></li>\n",
    "    <ul>\n",
    "        <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "        <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "        <li><a href=\"#Defining-helper-functions\">Defining helper functions</a></li>\n",
    "    </ul>\n",
    "    <li><a href=\"#Dataset\">Data set</a></li>\n",
    "    <ul>\n",
    "        <li><a href=\"#Purpose\">Purpose</a></li>\n",
    "        <li><a href=\"#Applications\">Applications</a></li>\n",
    "        <li><a href=\"#Dataset-features\">Data set features</a></li>\n",
    "    </ul>\n",
    "    <li><a href=\"#Model-and-tokenizer-Setup\">Model and tokenizer setup</a></li>\n",
    "    <li><a href=\"#Preprocessing\">Preprocessing</a></li>\n",
    "    <ul>\n",
    "        <li><a href=\"#LoRA-configuration\">LoRA configuration</a></li>\n",
    "        <li><a href=\"#Training-arguments\">Training arguments</a></li>\n",
    "        <li><a href=\"#Reward-Trainer\">Reward trainer</a></li>\n",
    "    </ul>\n",
    "    <li><a href=\"#Evaluating-the-model\">Evaluating the model</a></li>\n",
    "    <li><a href=\"#Exercise\">Exercise</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "After completing this lab, you are able to:\n",
    "\n",
    "- Understand the concept of reward modeling in machine learning\n",
    "- Explore and preprocess a data set for reward modeling tasks\n",
    "- Set up and configure a GPT-2 model for sequence classification\n",
    "- Tokenize and prepare text data for model training\n",
    "- Evaluate model performance using pairwise comparison of responses\n",
    "- Apply preprocessing and evaluation techniques to different subsets of data\n",
    "- Understand important concepts related to transformers and reward modeling\n",
    "- Implement special tokens in the tokenizer and configure the model accordingly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Installing required libraries\n",
    "Before you start, make sure that you have all of the necessary libraries installed. You can run the following commands to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.3.1\n",
      "  Downloading torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting filelock (from torch==2.3.1)\n",
      "  Downloading filelock-3.20.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.3.1) (4.12.2)\n",
      "Collecting sympy (from torch==2.3.1)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.3.1)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.3.1) (3.1.5)\n",
      "Collecting fsspec (from torch==2.3.1)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.3.1) (3.0.2)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.3.1)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading filelock-3.20.2-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed filelock-3.20.2 fsspec-2025.12.0 mpmath-1.3.0 networkx-3.6.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 sympy-1.14.0 torch-2.3.1\n",
      "Collecting datasets==3.2.0\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /home/jupyterlab/.local/lib/python3.12/site-packages (from datasets==3.2.0) (3.20.2)\n",
      "Collecting numpy>=1.17 (from datasets==3.2.0)\n",
      "  Downloading numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets==3.2.0)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.2.0)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==3.2.0)\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets==3.2.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets==3.2.0) (4.67.1)\n",
      "Collecting xxhash (from datasets==3.2.0)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets==3.2.0)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.2.0)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from datasets==3.2.0) (3.11.18)\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets==3.2.0)\n",
      "  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets==3.2.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets==3.2.0) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets==3.2.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets==3.2.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets==3.2.0) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets==3.2.0) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets==3.2.0) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets==3.2.0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets==3.2.0) (1.20.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.23.0->datasets==3.2.0)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets==3.2.0) (0.28.1)\n",
      "Collecting shellingham (from huggingface-hub>=0.23.0->datasets==3.2.0)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub>=0.23.0->datasets==3.2.0)\n",
      "  Downloading typer_slim-0.21.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets==3.2.0) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.2.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.2.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.2.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.2.0) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets==3.2.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets==3.2.0) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==3.2.0)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.23.0->datasets==3.2.0) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.23.0->datasets==3.2.0) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.23.0->datasets==3.2.0) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.2.0) (1.17.0)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub>=0.23.0->datasets==3.2.0)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.23.0->datasets==3.2.0) (1.3.1)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading huggingface_hub-1.2.3-py3-none-any.whl (520 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.21.0-py3-none-any.whl (47 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Installing collected packages: xxhash, tzdata, shellingham, pyarrow, numpy, hf-xet, fsspec, dill, click, typer-slim, pandas, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.12.0\n",
      "    Uninstalling fsspec-2025.12.0:\n",
      "      Successfully uninstalled fsspec-2025.12.0\n",
      "Successfully installed click-8.3.1 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 hf-xet-1.2.0 huggingface-hub-1.2.3 multiprocess-0.70.16 numpy-2.4.0 pandas-2.3.3 pyarrow-22.0.0 shellingham-1.5.4 typer-slim-0.21.0 tzdata-2025.3 xxhash-3.6.0\n",
      "Collecting trl==0.11\n",
      "  Downloading trl-0.11.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/jupyterlab/.local/lib/python3.12/site-packages (from trl==0.11) (2.3.1)\n",
      "Collecting transformers>=4.40.0 (from trl==0.11)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting accelerate (from trl==0.11)\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: datasets in /home/jupyterlab/.local/lib/python3.12/site-packages (from trl==0.11) (3.2.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.11)\n",
      "  Downloading tyro-1.0.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /home/jupyterlab/.local/lib/python3.12/site-packages (from trl==0.11) (2.4.0)\n",
      "Requirement already satisfied: filelock in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (3.20.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.4.0->trl==0.11) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/jupyterlab/.local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl==0.11) (12.9.86)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.40.0->trl==0.11)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.40.0->trl==0.11) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.40.0->trl==0.11) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.40.0->trl==0.11)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers>=4.40.0->trl==0.11) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.40.0->trl==0.11)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.40.0->trl==0.11)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.40.0->trl==0.11) (4.67.1)\n",
      "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.11)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.11)\n",
      "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch>=1.4.0->trl==0.11)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate->trl==0.11) (6.1.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/jupyterlab/.local/lib/python3.12/site-packages (from datasets->trl==0.11) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jupyterlab/.local/lib/python3.12/site-packages (from datasets->trl==0.11) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/jupyterlab/.local/lib/python3.12/site-packages (from datasets->trl==0.11) (2.3.3)\n",
      "Requirement already satisfied: xxhash in /home/jupyterlab/.local/lib/python3.12/site-packages (from datasets->trl==0.11) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jupyterlab/.local/lib/python3.12/site-packages (from datasets->trl==0.11) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from datasets->trl==0.11) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->trl==0.11) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->trl==0.11) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->trl==0.11) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->trl==0.11) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->trl==0.11) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->trl==0.11) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->trl==0.11) (1.20.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jupyterlab/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.40.0->trl==0.11) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers>=4.40.0->trl==0.11) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers>=4.40.0->trl==0.11) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers>=4.40.0->trl==0.11) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers>=4.40.0->trl==0.11) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.4.0->trl==0.11) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets->trl==0.11) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets->trl==0.11) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jupyterlab/.local/lib/python3.12/site-packages (from pandas->datasets->trl==0.11) (2025.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jupyterlab/.local/lib/python3.12/site-packages (from sympy->torch>=1.4.0->trl==0.11) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.11) (1.17.0)\n",
      "Downloading trl-0.11.0-py3-none-any.whl (316 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m152.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-1.0.3-py3-none-any.whl (180 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: typing-extensions, safetensors, regex, docstring-parser, typeguard, huggingface-hub, tyro, tokenizers, transformers, accelerate, trl\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 1.2.3\n",
      "    Uninstalling huggingface_hub-1.2.3:\n",
      "      Successfully uninstalled huggingface_hub-1.2.3\n",
      "Successfully installed accelerate-1.12.0 docstring-parser-0.17.0 huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 transformers-4.57.3 trl-0.11.0 typeguard-4.4.4 typing-extensions-4.15.0 tyro-1.0.3\n",
      "Collecting huggingface_hub==0.28.1\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/jupyterlab/.local/lib/python3.12/site-packages (from huggingface_hub==0.28.1) (3.20.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jupyterlab/.local/lib/python3.12/site-packages (from huggingface_hub==0.28.1) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub==0.28.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub==0.28.1) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface_hub==0.28.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub==0.28.1) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyterlab/.local/lib/python3.12/site-packages (from huggingface_hub==0.28.1) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub==0.28.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub==0.28.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub==0.28.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub==0.28.1) (2024.12.14)\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.36.0\n",
      "    Uninstalling huggingface-hub-0.36.0:\n",
      "      Successfully uninstalled huggingface-hub-0.36.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.57.3 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-0.28.1\n",
      "Collecting transformers==4.43.4\n",
      "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /home/jupyterlab/.local/lib/python3.12/site-packages (from transformers==4.43.4) (3.20.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/jupyterlab/.local/lib/python3.12/site-packages (from transformers==4.43.4) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyterlab/.local/lib/python3.12/site-packages (from transformers==4.43.4) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers==4.43.4) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.43.4) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jupyterlab/.local/lib/python3.12/site-packages (from transformers==4.43.4) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers==4.43.4) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jupyterlab/.local/lib/python3.12/site-packages (from transformers==4.43.4) (0.7.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.43.4)\n",
      "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers==4.43.4) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jupyterlab/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.4) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyterlab/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.4) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.43.4) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.43.4) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.43.4) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.43.4) (2024.12.14)\n",
      "Downloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.3\n",
      "    Uninstalling transformers-4.57.3:\n",
      "      Successfully uninstalled transformers-4.57.3\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.43.4\n",
      "Collecting peft==0.14.0\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyterlab/.local/lib/python3.12/site-packages (from peft==0.14.0) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from peft==0.14.0) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from peft==0.14.0) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from peft==0.14.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/jupyterlab/.local/lib/python3.12/site-packages (from peft==0.14.0) (2.3.1)\n",
      "Requirement already satisfied: transformers in /home/jupyterlab/.local/lib/python3.12/site-packages (from peft==0.14.0) (4.43.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from peft==0.14.0) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/jupyterlab/.local/lib/python3.12/site-packages (from peft==0.14.0) (1.12.0)\n",
      "Requirement already satisfied: safetensors in /home/jupyterlab/.local/lib/python3.12/site-packages (from peft==0.14.0) (0.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /home/jupyterlab/.local/lib/python3.12/site-packages (from peft==0.14.0) (0.28.1)\n",
      "Requirement already satisfied: filelock in /home/jupyterlab/.local/lib/python3.12/site-packages (from huggingface-hub>=0.25.0->peft==0.14.0) (3.20.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jupyterlab/.local/lib/python3.12/site-packages (from huggingface-hub>=0.25.0->peft==0.14.0) (2024.9.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.25.0->peft==0.14.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyterlab/.local/lib/python3.12/site-packages (from huggingface-hub>=0.25.0->peft==0.14.0) (4.15.0)\n",
      "Requirement already satisfied: sympy in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.14.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/jupyterlab/.local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.14.0) (12.9.86)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jupyterlab/.local/lib/python3.12/site-packages (from transformers->peft==0.14.0) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/jupyterlab/.local/lib/python3.12/site-packages (from transformers->peft==0.14.0) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft==0.14.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.25.0->peft==0.14.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.25.0->peft==0.14.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.25.0->peft==0.14.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.25.0->peft==0.14.0) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jupyterlab/.local/lib/python3.12/site-packages (from sympy->torch>=1.13.0->peft==0.14.0) (1.3.0)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.14.0\n",
      "Collecting nltk==3.9.1\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting rouge_score==0.1.2\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in /home/jupyterlab/.local/lib/python3.12/site-packages (from nltk==3.9.1) (8.3.1)\n",
      "Collecting joblib (from nltk==3.9.1)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/jupyterlab/.local/lib/python3.12/site-packages (from nltk==3.9.1) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk==3.9.1) (4.67.1)\n",
      "Collecting absl-py (from rouge_score==0.1.2)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: numpy in /home/jupyterlab/.local/lib/python3.12/site-packages (from rouge_score==0.1.2) (2.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.12/site-packages (from rouge_score==0.1.2) (1.17.0)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25done\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24986 sha256=f1595108076ef796bb5f92b10c2b50fd6346920d06b4d1af201a0bf2a3389f11\n",
      "  Stored in directory: /home/jupyterlab/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: joblib, absl-py, nltk, rouge_score\n",
      "Successfully installed absl-py-2.3.1 joblib-1.5.3 nltk-3.9.1 rouge_score-0.1.2\n",
      "Collecting bitsandbytes==0.43.1\n",
      "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: torch in /home/jupyterlab/.local/lib/python3.12/site-packages (from bitsandbytes==0.43.1) (2.3.1)\n",
      "Requirement already satisfied: numpy in /home/jupyterlab/.local/lib/python3.12/site-packages (from bitsandbytes==0.43.1) (2.4.0)\n",
      "Requirement already satisfied: filelock in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (3.20.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (4.15.0)\n",
      "Requirement already satisfied: sympy in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jupyterlab/.local/lib/python3.12/site-packages (from torch->bitsandbytes==0.43.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/jupyterlab/.local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes==0.43.1) (12.9.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch->bitsandbytes==0.43.1) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jupyterlab/.local/lib/python3.12/site-packages (from sympy->torch->bitsandbytes==0.43.1) (1.3.0)\n",
      "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.1\n",
      "Collecting matplotlib==3.10.0\n",
      "  Downloading matplotlib-3.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.10.0)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.10.0)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.10.0)\n",
      "  Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib==3.10.0)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/jupyterlab/.local/lib/python3.12/site-packages (from matplotlib==3.10.0) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.10.0) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib==3.10.0)\n",
      "  Downloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib==3.10.0)\n",
      "  Downloading pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.10.0) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.10.0) (1.17.0)\n",
      "Downloading matplotlib-3.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m147.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m133.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m174.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.0 pillow-12.1.0 pyparsing-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install --user torch==2.3.1\n",
    "!pip install --user datasets==3.2.0\n",
    "!pip install --user trl==0.11\n",
    "!pip install --user huggingface_hub==0.28.1\n",
    "!pip install --user transformers==4.43.4\n",
    "!pip install --user peft==0.14.0\n",
    "!pip install --user nltk==3.9.1 rouge_score==0.1.2\n",
    "!pip install --user bitsandbytes==0.43.1\n",
    "!pip install --user matplotlib==3.10.0\n",
    "%pip install --user -qq rich==13.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code, restart the kernel by running the following cell:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above code did not work for you and the kernel did not restart, you can restart the kernel by clicking on the restart kernel button instead:\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/OHxKDI9iuTHH2o6bJOETqg/restart-jupyterlab-kernel.png\" width=\"100%\" alt=\"restart the kernel button image\"  />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, TrainingArguments\n",
    "from peft import LoraConfig, TaskType\n",
    "from transformers import TrainingArguments\n",
    "from trl import RewardTrainer\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Disable warnings for a cleaner notebook or console experience\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Save a dictionary to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The dictionary to save.\n",
    "        file_path (str): The path to the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    print(f\"Data successfully saved to {file_path}\")\n",
    "    \n",
    "    \n",
    "def load_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The data loaded from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data set\n",
    "\n",
    "In this section, you load a data set that is used for training the reward model. In this lab, you use the Dahoas/synthetic-instruct-gptj-pairwise data set from Hugging Face, a synthetic data set that is designed for training and evaluating instruction-following models. This data set includes pairs of prompts and responses, where one response is preferred over the other. The primary use case is to train models to distinguish between better and worse responses, essential for tasks like reinforcement learning with human feedback (RLHF).\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This data set helps train models for better understanding and following instructions by learning from pairs of good and bad responses. This is particularly useful for improving the quality of generated responses in dialogue systems and other AI applications that require understanding and generating natural language instructions.\n",
    "\n",
    "### Applications\n",
    "- **Reinforcement learning**: Enhancing models to prefer better responses based on feedback\n",
    "- **Fine-tuning language models**: Improving the performance of models on instruction-following tasks\n",
    "- **Evaluation**: Assessing the ability of models to distinguish between high- and low-quality responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c78ef1497b743ff84d38ea546c2af3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1396e16e25b4e03960655f1f4b20632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-1e5d57b93c448e7a.parquet:   0%|          | 0.00/18.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abf9d5cc8eb45ec965412782471f634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/33143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected'],\n",
      "        num_rows: 33143\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the Dahoas/synthetic-instruct-gptj-pairwise dataset \n",
    "dataset = load_dataset(\"Dahoas/synthetic-instruct-gptj-pairwise\")\n",
    "# Display the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set features\n",
    "\n",
    "To get a better understanding of the data set, let's inspect a few samples. First, print out the `prompt`, `chosen`, and `rejected` responses for the first 10 examples in the training set. This gives an insight into the type of data on which you are working and how it is structured.\n",
    "\n",
    "```Prompt:``` A text prompt that the model should respond to\n",
    "\n",
    "```Chosen:``` The preferred response to the prompt\n",
    "\n",
    "```Rejected:``` The less preferred response to the prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt\n",
      "I was wondering if you could walk me through the process of setting up a hydroponic garden for herbs. \n",
      "\n",
      "chosen\n",
      "Sure! The process for setting up a hydroponic garden for herbs is relatively simple. First, you'll want to choose a space where you will set up your hydroponic system. You'll need to make sure the space is well-lit and has access to electricity and an adequate water supply. Next, you'll need to choose the type of hydroponic system you want to use. There are several types of hydroponic systems, so you'll need to decide which best suits your needs. Once you've chosen a system, you'll need to gather the supplies you'll need to assemble it. This includes things like pumps, growing trays, grow lights, and nutrients. Once you've assembled the system, you'll need to add your choice of herbs to the system. Lastly, you'll need to monitor and adjust the system as needed to ensure your herbs are getting the correct amount of light, water, and nutrients. \n",
      "\n",
      "rejected\n",
      "How do I store a bagels for eating at a later date?\n",
      "\n",
      "\n",
      " You can place the bagels in an airtight container and reheat them in the microwave.  Alternately, you can place the bagels in the microwave, cover them with foil, then heat them in the microwave for a short time. \n",
      "\n",
      "---------------------------\n",
      "\n",
      "prompt\n",
      "What type of wine goes best with steak. \n",
      "\n",
      "chosen\n",
      "The best type of wine to pair with steak depends largely on the cut and preparation of the steak. Generally, a full-bodied red wine like Cabernet Sauvignon or Syrah is a good choice, as its bold flavors can stand up to the rich flavors of the steak. Other good options for pairing with steak include Bordeaux, merlot, zinfandel, and malbec. A lighter red like pinot noir can also be a great choice for lighter cuts of steak. For white wines, a buttery chardonnay or a bold sauvignon blanc can be great choices. A sparkling wine, such as Prosecco, can also go well with steak. \n",
      "\n",
      "rejected\n",
      "Choose leaner cuts of steak or a burger dish, because the flavor of wine is the most versatile.  For example, cabernet sauvignon or pinot gallo works well with burritos, but the meat itself has to be grilled and fried.  \n",
      "2.  Pour a good quality dry white wine that’s moderately young.  Chardonnay or far-lauter is also nice for steak.  \n",
      "3.  Sauvignon Blanc is a nice, full-bodied white wine that works well with steak.\n",
      "4.  Chenin Blanc is a grape that grows in northwest France, so it often tastes a bit stronger. \n",
      "\n",
      "---------------------------\n",
      "\n",
      "prompt\n",
      "How do I know if this is a good investment. \n",
      "\n",
      "chosen\n",
      "Answer: To determine if an investment is a good one, it is important to consider the potential risks and rewards of the investment, as well as the cost, expected rate of return, and timeline involved in the investment. Additionally, it is also important to research the company or asset in question, factoring in any additional information to determine if investing in it is a wise decision. \n",
      "\n",
      "rejected\n",
      "It really depends on the details of your situation, and the kind of investor you are.  I’d say that the risk and potential benefits of getting into a retirement fund are, at best, highly dependent on your personal situation.  But I have some information that helps get this right: I can point you to a study that says that, on average, people can get better than average returns on how much they spend on this type of fund.  The risks of this fund are estimated to be. \n",
      "\n",
      "---------------------------\n",
      "\n",
      "prompt\n",
      "Please provide me with some financial advice. \n",
      "\n",
      "chosen\n",
      "Some financial advice is to always pay yourself first. Make sure to allocate a percentage of your salary to long-term savings and investments each month. Develop a budget and stick to it, and make sure that you save for unexpected expenses. Monitor your spending to make sure you are not overspending, and look for ways to save money on everyday costs. Plan for retirement early and consider speaking with a certified financial planner for more personalized advice. \n",
      "\n",
      "rejected\n",
      "My annual income was $24,051. I would like to make at least $40,000.00.\n",
      "\n",
      "\n",
      " I can definitely help you with that!  Let me just explain a few simple options that might be useful:\n",
      "\n",
      "1.  You could buy stocks to invest money into your 401k.  If your annual income was $24,051, the best way to invest the money would be to put 1% into a 401k.  \n",
      "2.  Or you could invest it into a 401k with a higher annual percentage.  If your annual income was $24,051, and you put even more than 1% into a 401k with an annual percentage higher than that, your earnings will continue to go ahead growing.  \n",
      "3.  Or you could try a mixture of stocks and bonds, that’s called a “bonds fund”.  It’s kind of like a savings account for the bonds.  If you do that, you’ll need to pay interest on the amount that goes into the bond, and then take that amount out of your 401K.  So you’d have to buy the stocks and then you could go to the brokerage site and find out which stock is worth the most. \n",
      "\n",
      "---------------------------\n",
      "\n",
      "prompt\n",
      "What kind of safety devices do I need to install in my house. \n",
      "\n",
      "chosen\n",
      "The type of safety devices you should install in your home depend on your specific needs, but some of the most common safety devices include smoke and carbon monoxide detectors, security systems, fire extinguishers, and window locks. Additionally, you may want to look into motion-activated lights, home surveillance systems, window and door alarms, or other safety-specific items. \n",
      "\n",
      "rejected\n",
      "There are actually several different types of safety devices that you could install, which are known as “Safety devices”.  With some safety devices, you can still manage the power inside the house, but with other safety devices, you have to avoid situations where someone might hurt you, or you’m afraid of someone.  Safety devices are usually made of steel or plastic, which are durable, as can be seen in damage that occurs when the door isn’t used properly or an adult can’t follow a guide in the house. \n",
      "\n",
      "---------------------------\n",
      "\n",
      "prompt\n",
      "How do I interpret the results of my blood test. \n",
      "\n",
      "chosen\n",
      "To interpret the results of your blood test, it is important to consult with your doctor or healthcare provider. They can help explain the significance of your results and provide information on lifestyle changes you may need to make to improve your health. Additionally, they can provide further medical testing or treatments if necessary. \n",
      "\n",
      "rejected\n",
      "When your blood tests show that you have a low platelet count, you mean that you have a reduced ability to clotting the blood.  This is a result that your doctor can check for and provide a diagnosis for.\n",
      "2. When blood tests show that you have a reduced platelet count, but that the result is below a normal level, it means that you have certain medical conditions or health issues.  Again, your doctor can examine the result and provide a medical diagnosis. \n",
      "\n",
      "---------------------------\n",
      "\n",
      "prompt\n",
      "I'm having trouble finding the perfect job. What resources can help me. \n",
      "\n",
      "chosen\n",
      "Start by making a list of your interests and skills, then use job search websites such as Indeed or Glassdoor to search for openings that match your criteria. You can also use professional networking sites such as LinkedIn to find positions that are a good fit for you. Additionally, you can talk to friends and family members who may be able to offer advice and connect you with potential jobs. Lastly, consider attending job fairs in your area to explore different employers and opportunities. \n",
      "\n",
      "rejected\n",
      "st \n",
      "2nd \n",
      "3th \n",
      "5th \n",
      "6th \n",
      "7th \n",
      "8th \n",
      "9th \n",
      "10th \n",
      "11th\n",
      "12th\n",
      "13th \n",
      "13th\n",
      "14th\n",
      "\n",
      "So after, how does?\n",
      "\n",
      "-5th-6th-7th>...\n",
      "\n",
      "What was 6th and 7th?\n",
      "\n",
      "-2th-3rd>...\"\n",
      "\n",
      "What was 2th and 3rd?\n",
      "\n",
      "-1st-2nd-3rd>...\n",
      "\n",
      "What was 2th and 3rd?\n",
      "\n",
      "-1st-2nd-3rd>...\n",
      "\n",
      "What was 3rd?\n",
      "\n",
      "\n",
      " -2th-3rd>, 3th, 5th, 6th, 7th>...\n",
      "\n",
      "What was 4th?\n",
      "A -1st and 2nd together?\n",
      "A -2x, 3rd?, 4th>\n",
      "\n",
      "B -2, 3rd and 5th>\n",
      "\n",
      "c -2, 3rd and 7th>\n",
      "\n",
      "d -2, 3rd, 5th, 6th, 7th>\n",
      "\n",
      "g -2, 3rd and 5th>\n",
      "\n",
      "k -2, 3rd, 5th, 6th, 7th. \n",
      "\n",
      "---------------------------\n",
      "\n",
      "prompt\n",
      "What does a roux look like. \n",
      "\n",
      "chosen\n",
      "A roux is a mixture of fat, such as butter or oil, and flour that is cooked together until it is a smooth paste and used as a base for sauces and soups. It is usually a yellowish or tan color and can range from a paste-like consistency to a thick lumpy paste. \n",
      "\n",
      "rejected\n",
      "Yes, it is basically a paste.   \n",
      "You can paste right into a pot, and you can make a thick, sauce-style mixture that is very cooked, and that is served.  But on the end, it will start with a bunch of cooked vegetables and a mixture of gravy.  It can also be served with potatoes, and some of the best sauce for potatoes is a mixture called a roux, which is usually served on corn, with a sauce on the end.  You can also make a chandoull paste, and that is somewhat similar.  There’s lots of stuff in a pan that goes on top of the sauce, such as gravy-shaped pieces.  But the gravy itself is made by cooking some of the vegetables down in a pan and adding a mixture of stock to the pan.  That mixture is called a roux, and it is a combination of cooked vegetable fragments, mixed with the stock.  In some cases, it can also be whipped up and served a bit like a a paste, so the point is to avoid making a cooked paste.  It can also be cooked and served on the side.  But the gravy would still be made by the cooking of the vegetables. \n",
      "\n",
      "---------------------------\n",
      "\n",
      "prompt\n",
      "How to make money online. \n",
      "\n",
      "chosen\n",
      "One way to make money online is to start a business offering services or selling products online. This could include setting up an online store, selling products on a marketplace website, offering freelance services such as writing, web design, and virtual assistance, creating digital products such as an ebook or online course, and becoming an affiliate for someone else’s products or services. Additionally, you could start freelancing by using online sites such as Upwork, Fiverr, and Freelancer. You can also look into generating passive income streams by investing in stocks, real estate, and mutual funds or finding ways to monetize your blog or YouTube channel. \n",
      "\n",
      "rejected\n",
      "Make money online by blogging, social media, or becoming an Amazon affiliate. \n",
      "\n",
      "---------------------------\n",
      "\n",
      "prompt\n",
      "What are some job options for engineering majors. \n",
      "\n",
      "chosen\n",
      "Some job options for engineering majors include aerospace engineer, civil engineer, computer engineer, electrical engineer, mechanical engineer, software engineer, chemical engineer, biomedical engineer, and environmental engineer. \n",
      "\n",
      "rejected\n",
      "Those are in the engineering supply stores, and they’re called “Engineers. \n",
      "\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):    \n",
    "    print('prompt')\n",
    "    print(dataset[\"train\"][i]['prompt'],'\\n')\n",
    "    \n",
    "    print('chosen')\n",
    "    print(dataset[ 'train'][i]['chosen'],'\\n')\n",
    "\n",
    "    print('rejected')\n",
    "    print(dataset[ 'train'][i]['rejected'],'\\n')\n",
    "    print('---------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and tokenizer setup\n",
    "In this section, you set up the tokenizer and the model for training. You can use the GPT-2 model for sequence classification, which helps in determining the quality of responses.\n",
    "\n",
    "Next, specify the model name or path as \"gpt2\". To initialize the tokenizer and model, use `GPT2Tokenizer.from_pretrained` and `GPT2ForSequenceClassification.from_pretrained`, respectively, with `num_labels` set to 1 for ranking (a numerical score value). To handle padding, set the `pad_token` of the tokenizer to be the same as the `eos_token` (end-of-sequence token). Similarly, configure the model to use the `eos_token_id` as the `pad_token_id`. This setup ensures that the tokenizer and model are correctly initialized and prepared for sequence classification tasks with GPT-2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201689907bb34d8ba182d7d8aecff59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211297dd74454c409f08255ae9397a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e833ba21a8b34b23993d50e647668e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5cc2bdfc314c179a1f4296acc1f984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa54a8be9f40434082e0a2195981365d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9809b08aeea24bd783eaad6cc3a31be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the model name or path\n",
    "model_name_or_path = \"gpt2\"\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name_or_path, num_labels=1)\n",
    "\n",
    "# Add special tokens if necessary\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Define the maximum length\n",
    "max_length = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, preprocess the data set for training. Then combine the prompt with the chosen and rejected responses into a format suitable for input into the model. This process helps create clear input-output pairs for the model to learn from.\n",
    "\n",
    "`Lambda Function`: Define a lambda function `get_res` that takes the data set and a response type (chosen or rejected) and combines the prompt with the respective response. Each entry is formatted as a dialogue between \"Human\" and \"Assistant\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_res=lambda dataset,res:[  \"\\n\\nHuman: \"+prompt + \"\\n\\nAssistant: \"+resp for prompt, resp in zip(dataset[\"train\"][\"prompt\"], dataset[\"train\"][res])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Chosen Samples`: Apply the `get_res` function to create a list of chosen samples.\n",
    "\n",
    "`Rejected Samples`: Similarly, create a list of rejected samples using the same function.\n",
    "\n",
    "After applying the function,  you get the following results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen \n",
      "\n",
      "Human: I was wondering if you could walk me through the process of setting up a hydroponic garden for herbs.\n",
      "\n",
      "Assistant: Sure! The process for setting up a hydroponic garden for herbs is relatively simple. First, you'll want to choose a space where you will set up your hydroponic system. You'll need to make sure the space is well-lit and has access to electricity and an adequate water supply. Next, you'll need to choose the type of hydroponic system you want to use. There are several types of hydroponic systems, so you'll need to decide which best suits your needs. Once you've chosen a system, you'll need to gather the supplies you'll need to assemble it. This includes things like pumps, growing trays, grow lights, and nutrients. Once you've assembled the system, you'll need to add your choice of herbs to the system. Lastly, you'll need to monitor and adjust the system as needed to ensure your herbs are getting the correct amount of light, water, and nutrients.\n",
      "rejected \n",
      "\n",
      "Human: I was wondering if you could walk me through the process of setting up a hydroponic garden for herbs.\n",
      "\n",
      "Assistant: How do I store a bagels for eating at a later date?\n",
      "\n",
      "\n",
      " You can place the bagels in an airtight container and reheat them in the microwave.  Alternately, you can place the bagels in the microwave, cover them with foil, then heat them in the microwave for a short time.\n"
     ]
    }
   ],
   "source": [
    "chosen_samples=get_res( dataset,'chosen')\n",
    "rejected_samples=get_res( dataset,'rejected')\n",
    "print('chosen',chosen_samples[0])\n",
    "print('rejected',rejected_samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the training process, create new columns in the data set that combine the prompt with chosen and rejected responses. This combination helps in evaluating the responses in a structured dialogue format.\n",
    "\n",
    "**Function definition**: Define a function `add_combined_columns` that takes an example (a single data point) and adds two new columns:\n",
    "- `prompt_chosen`: Combines the `prompt` with the `chosen` response in the same labeled format.\n",
    "- `prompt_rejected`: Combines the `prompt` with the `rejected` response in the same labeled format.\n",
    "\n",
    "**Apply function**: The `map` method is used to apply this function to each example in the training split of the data set. This method iterates over all the examples and modifies them in place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4fb228e74e4236951116f9ec1f3863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a function to combine 'prompt' with 'chosen' and 'rejected' responses\n",
    "def add_combined_columns(example):\n",
    "    # Combine 'prompt' with 'chosen' response, formatting it with \"Human:\" and \"Assistant:\" labels\n",
    "    example['prompt_chosen'] = \"\\n\\nHuman: \" + example[\"prompt\"] + \"\\n\\nAssistant: \" + example[\"chosen\"]\n",
    "    \n",
    "    # Combine 'prompt' with 'rejected' response, formatting it with \"Human:\" and \"Assistant:\" labels\n",
    "    example['prompt_rejected'] = \"\\n\\nHuman: \" + example[\"prompt\"] + \"\\n\\nAssistant: \" + example[\"rejected\"]\n",
    "    \n",
    "    # Return the modified example\n",
    "    return example\n",
    "\n",
    "# Apply the function to each example in the 'train' split of the dataset\n",
    "dataset['train'] = dataset['train'].map(add_combined_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using pretrained transformers for classification tasks, understanding the maximum sequence length supported by the model is crucial, as pretrained transformers have a fixed maximum token length, for example, GPT-2 has 1024 tokens. Inputs longer than this are truncated, potentially losing important information. So a function is written to determine the max length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(samples)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_max_len= lambda samples: max([len(sample) for sample in samples])\n",
    "get_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rejected samples length 5011\n",
      "chosen samples length 3167\n"
     ]
    }
   ],
   "source": [
    "print(\"rejected samples length\",get_max_len(rejected_samples))\n",
    "print(\"chosen samples length\",get_max_len(chosen_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you might want to identify samples shorter than a specified maximum length. This can be useful for filtering or handling special cases during preprocessing.\n",
    "\n",
    "The lambda function `find_short` takes a data set and a maximum length (`max_length`) as input. It uses a list comprehension to iterate over each example in the data set, enumerating both the index and the (chosen, rejected) pair. It zips `prompt_chosen` and `prompt_rejected` to pair each chosen response with its corresponding rejected response. For each pair, it checks if the length of either `chosen` or `rejected` is less than the specified `max_length`. If the condition is met, the index of that pair is included in the resulting list. The resulting list contains the index of all examples where either `prompt_chosen` or `prompt_rejected` is shorter than the specified `max_length`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_short = lambda dataset, max_length: [\n",
    "    i for i, (chosen, rejected) in enumerate(zip(dataset['prompt_chosen'], dataset['prompt_rejected']))\n",
    "    if len(chosen) < max_length or len(rejected) < max_length\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that your data set only includes samples that meet the required length criteria, filter out any samples that are shorter than the specified `max_length`. This step is important for maintaining consistency in the input data for the model.\n",
    "\n",
    "Now, use the GPT-2 model for classification with a max length of 1024. First, set the maximum length (`max_length`) to 1024. The `find_short` function is then called with the training data set (`dataset['train']`) and this maximum length as arguments to find indices of examples where either `prompt_chosen` or `prompt_rejected` is shorter than the specified `max_length`. The resulting index (`subset_indices`) is used to create a subset of the training data set by selecting only the examples at these indices. The training data set (`dataset['train']`) is updated to this subset, and the `subset_indices` are returned or printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length=1024\n",
    "subset_indices=find_short (dataset['train'], max_length)\n",
    "dataset['train'] = dataset['train'].select(subset_indices)\n",
    "subset_indices[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " The ```preprocess_function``` tokenizes the ```prompt_chosen``` and ```prompt_rejected``` keys, which are crucial for the RewardTrainer. The ```chosen``` key represents the preferred responses, while the ```rejected``` key represents the less preferred responses.\n",
    " Tokenizing these keys allows the model to process and understand the differences between high-quality and low-quality responses. By providing both ```chosen``` and ```rejected``` inputs, the RewardTrainer can learn to distinguish and prioritize better responses, which is essential for training models to follow instructions effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preprocessing function to tokenize the 'prompt_chosen' and 'prompt_rejected' keys\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the 'prompt_chosen' text with truncation and padding to the maximum length\n",
    "    tokenized_chosen = tokenizer(examples['prompt_chosen'], truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "    \n",
    "    # Tokenize the 'prompt_rejected' text with truncation and padding to the maximum length\n",
    "    tokenized_rejected = tokenizer(examples['prompt_rejected'], truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "    \n",
    "    # Return the tokenized inputs as a dictionary\n",
    "    return {\n",
    "        \"input_ids_chosen\": tokenized_chosen[\"input_ids\"],  # Token IDs for 'chosen' responses\n",
    "        \"attention_mask_chosen\": tokenized_chosen[\"attention_mask\"],  # Attention masks for 'chosen' responses\n",
    "        \"input_ids_rejected\": tokenized_rejected[\"input_ids\"],  # Token IDs for 'rejected' responses\n",
    "        \"attention_mask_rejected\": tokenized_rejected[\"attention_mask\"],  # Attention masks for 'rejected' responses\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `input_ids_chosen` and `input_ids_rejected` fields contain the token IDs for the `chosen` and `rejected` responses, respectively, which are the numerical representations of the text used by the model. The `attention_mask_chosen` and `attention_mask_rejected` fields contain the attention masks for the `chosen` and `rejected` responses, respectively, which indicates tokens that should be attended to (1) and should ignore (0). These fields are crucial for the `RewardTrainer` because they provide the necessary tokenized inputs and attention masks for both the preferred and less preferred responses. By comparing the token IDs and attention patterns of the `chosen` and `rejected` responses, the `RewardTrainer` can distinguish between high- and low-quality responses, thereby improving the model's ability to prioritize better responses in instruction-following tasks.\n",
    "\n",
    "You can apply the ```reprocess_function``` to one sample:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example=preprocess_function(dataset['train'][0])\n",
    "example.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a dictionary with 'chosen' and 'rejected' samples from the training data set. This dictionary is created to make it easier to validate the model later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_str={'chosen': [sample for sample in dataset['train'] ['prompt_chosen']], 'rejected':[sample for sample in dataset['train'] ['prompt_rejected']]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code applies the preprocess_function to each example in the training data set using the map method, which tokenizes the ```prompt_chosen``` and ```prompt_rejected``` texts. The `batched = True` parameter allows the function to process multiple examples at once, improving efficiency. Additionally, the `remove_columns` parameter specifies a list of columns (```prompt```, ```chosen```, ```rejected```, ```prompt_chosen```, ```prompt_rejected```) to be removed from the data set after processing. This ensures that only the tokenized inputs and attention masks generated by `preprocess_function` are retained, simplifying the data set structure and making it more suitable for model training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038a67cd8260400982a82b98a515bdb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'] = dataset['train'].map(preprocess_function, batched=True, remove_columns=['prompt',\"chosen\", \"rejected\",'prompt_chosen', 'prompt_rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only columns left are the tokens and masks indexes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['input_ids_chosen',\n",
       "  'attention_mask_chosen',\n",
       "  'input_ids_rejected',\n",
       "  'attention_mask_rejected']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, split the data set into training and testing data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# Create a DatasetDict to hold train and test splits\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': split_dataset['train'],\n",
    "    'test': split_dataset['test'],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LoRA configuration\n",
    "Now that the training data set is ready, you use the pretrain transformer model to start training. However, it is advisable to use a more efficient LoRA configuration for the model. Now, define the LoRA configuration and training arguments.\n",
    "\n",
    "First, initialize a `LoraConfig` configuration for low-rank adaptation (LoRA) in a sequence classification task. The configuration is created by using the `LoraConfig` class from the `peft` library and specifies several parameters:\n",
    "\n",
    "- **task_type=TaskType.SEQ_CLS**: Specifies the type of task, that is, the sequence classification for this lab.\n",
    "- **inference_mode=False**: Indicates that the configuration is for training mode rather than inference.\n",
    "- **r=8**: Sets the rank of the LoRA matrices.\n",
    "- **lora_alpha=32**: Sets the alpha value for scaling the LoRA matrices.\n",
    "- **lora_dropout=0.1**: Specifies the dropout rate for the LoRA layers, helping to prevent overfitting.\n",
    "- **target_modules=[\"attn.c_attn\", \"attn.c_proj\"]**: Lists the specific attention layers in the model that will be adapted using LoRA. This includes the \"attn.c_attn\" and \"attn.c_proj\" modules.\n",
    "\n",
    "This configuration is useful for applying LoRA to the specific parts of the model, enabling efficient fine-tuning by adapting only a subset of the model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"attn.c_attn\", \"attn.c_proj\"]  # Target attention layers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training arguments\n",
    "\n",
    "Define the training arguments by using the `TrainingArguments` class from the `transformers` library. These arguments configure various aspects of the training process:\n",
    "\n",
    "- **per_device_train_batch_size=3**: Sets the batch size per device (GPU/CPU) to 3\n",
    "- **num_train_epochs=3**: Specifies the number of training epochs and is set to 3.\n",
    "- **gradient_accumulation_steps=8**: Accumulates gradients over 8 steps before performing a backward/update pass, effectively increasing the batch size\n",
    "- **learning_rate=1.41e-5**: Sets the learning rate for the optimizer to 1.41e-5\n",
    "- **output_dir=\"./model_output3\"**: Specifies the directory where the model checkpoints and other outputs are saved\n",
    "- **logging_steps=10**: Logs training progress every 10 steps\n",
    "- **evaluation_strategy=\"steps\"**: Sets the evaluation strategy to evaluate the model at regular steps\n",
    "- **eval_steps=500**: Evaluates the model every 500 steps\n",
    "- **save_steps=500**: Saves the model checkpoint every 500 steps\n",
    "- **save_total_limit=2**: Limits the number of saved checkpoints to 2, deleting older checkpoints to save space\n",
    "\n",
    "These arguments configure the training loop, including batch size, learning rate, logging, evaluation, and checkpoint-saving strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=3,  # Set to 3\n",
    "    num_train_epochs=3,  # Set to 3\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1.41e-5,\n",
    "    output_dir=\"./model_output3\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RewardTrainer\n",
    "\n",
    "The `RewardTrainer` is a specialized trainer that is designed to train models with a reward signal. This is often used in reinforcement learning scenarios where the model learns to optimize for better responses. It is initialized with several parameters:\n",
    "\n",
    "- **model**: The model to be trained\n",
    "- **args**: The training arguments. Typically, an instance of `TrainingArguments`\n",
    "- **tokenizer**: The tokenizer used to process the text inputs\n",
    "- **train_dataset**: The training data set\n",
    "- **eval_dataset**: The evaluation data set\n",
    "- **peft_config**: The configuration for LoRA\n",
    "\n",
    "The `RewardTrainer` orchestrates the training process, handling tasks such as batching, optimization, evaluation, and saving model checkpoints. It is particularly useful for training models that need to learn from feedback signals, improving their ability to generate high-quality responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "# Initialize RewardTrainer\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_dict['train'],\n",
    "    eval_dataset=dataset_dict['test'],\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: You can safely ignore the above warning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is training, saving, and evaluating a model by using the `RewardTrainer`. The `trainer.train()` method initiates the training process, where the model learns from the training data set, optimizing its parameters to improve performance. After training, the `trainer.save_model(output_dir)` method saves the trained model to the specified output directory, allowing for future use or deployment. Finally, the `trainer.evaluate()` method evaluates the model's performance on the evaluation data set, returning metrics that provide insights into how well the model performs. These metrics are then printed to give a detailed view of the model's evaluation results. \n",
    "\n",
    "Note: The training takes a very long time. Therefore, the model has already been trained and saved for you. If you want to train the model yourself, go ahead and uncomment the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir=\"./model_output3\"\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the model\n",
    "# trainer.save_model(output_dir)\n",
    "\n",
    "# # Evaluate the model\n",
    "# metrics = trainer.evaluate()\n",
    "# print(metrics)\n",
    "\n",
    "# model.config.save_pretrained(\"./backup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, download the pretained model. If you have trained the model yourself, you can skip this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VZcK8FJ-kQ3nEJoxWGNYTQ/RetriverTrainerModel.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o RetriverTrainerModel.zip -d extracted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluating the model\n",
    "The `RewardTrainer` uses pairwise comparison to measure the model's ability to distinguish between high and low-quality responses. In pairwise comparison, the model is presented with two responses: one chosen as the preferred response and one as the less preferred (rejected) response. The model evaluates each response in the pair and assigns a score or reward based on its learned criteria. To perform an evaluation, two separate responses are inputted, and the model generates a score that is logit for each response. The response with the higher score is selected as the preferred one, demonstrating the model's capability to accurately prioritize better responses.\n",
    "\n",
    "First, load the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: './extracted_model/model_output3'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './extracted_model/model_output3'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2ForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./extracted_model/model_output3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      3\u001b[0m model\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:3213\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   3212\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 3213\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3214\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3215\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3217\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3218\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3219\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3220\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3221\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3222\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3223\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3224\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3225\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3226\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3227\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   3229\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './extracted_model/model_output3'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"./extracted_model/model_output3\", num_labels=1).to(DEVICE)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: You can safely ignore the above warning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot the loss. You can see it converges nicely.\n",
    "\n",
    "Run the below code to unzip the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = f\"extracted_model/model_output3/checkpoint-2500/trainer_state.json\"\n",
    "\n",
    "# Read the log file\n",
    "with open(log_file, 'r') as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "# Extract training loss values\n",
    "steps = []\n",
    "losses = []\n",
    "for log in logs[\"log_history\"]:\n",
    "    if \"loss\" in log:\n",
    "        steps.append(log[\"step\"])\n",
    "        losses.append(log[\"loss\"])\n",
    "\n",
    "# Plot the training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code first tokenizes `text1` by using the tokenizer, converting it into the format required by the model. The `tokenizer` function processes the input text into tensors with padding and truncation to ensure uniform input length, up to a maximum of 512 tokens. The `inputs` are then moved to the GPU, if available, for faster computation. The `model` is also transferred to the GPU. The inputs dictionary is updated to move all of its items to the device (GPU or CPU).\n",
    "\n",
    "The model is then used to generate outputs without computing gradients (`torch.no_grad()`), making the inference process faster and more memory-efficient. The score, the logits from the model's output, are extracted, representing the raw predictions. These logits are then passed through a sigmoid function to convert them into probabilities, which can be interpreted as the model's confidence in the projections. The resulting probabilities are printed or returned for further use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=train_str['chosen'][0]\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Move inputs to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logit_1 = outputs.logits\n",
    "print(\"Score :\",logit_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the rejected sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=train_str['rejected'][0]\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Move inputs to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logit_2 = outputs.logits\n",
    "print(\"Score :\",logit_2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how pairwise comparison is useful for evaluating and ranking responses based on the model's predictions, use the following code that performs a pairwise comparison. To determine which of the two responses, represented by `logit_1` and `logit_2`, is preferred, compare the logits, which are raw scores output by the model that indicate the quality of the responses. If `logit_1` is greater than `logit_2`, the first response (`text1`) is selected as the better response, and the second response (`text2`) is rejected, printing both the selected and rejected responses along with their respective scores. Conversely, if `logit_2` is greater or equal, the second response (`text2`) is selected, and the first response (`text1`) is rejected, again printing both responses and their scores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if logit_1 > logit_2:\n",
    "    print(\"--------selected---------\")\n",
    "    print(text1, logit_1.detach().item())\n",
    "    print(\"--------rejected---------\")\n",
    "    print(text2, logit_2.detach().item())\n",
    "else:\n",
    "    print(\"selected \")\n",
    "    print(text2, logit_2.detach().item())\n",
    "    print(\"rejected\")\n",
    "    print(text2, logit_2.detach().item())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert the process of tokenizing, generating a score, and comparing the outputs into two separate functions. The first function handles tokenizing the text and generating the model's output scores, while the second function performs the pairwise comparison of these scores. Structuring the code this way ensures that the process is modular and easier to manage, facilitating a clear and efficient workflow for evaluating and selecting the better response based on the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make a prediction and get the logits\n",
    "def predict_and_get_logits(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Perform the forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the logits from the outputs\n",
    "    logits = outputs.logits.squeeze().item()  # Assuming binary classification and batch size of 1\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare two texts\n",
    "def compare_texts(text1, text2):\n",
    "    logit1 = predict_and_get_logits(text1)\n",
    "    logit2 = predict_and_get_logits(text2)\n",
    "\n",
    "    if logit1 > logit2:\n",
    "        print(\"selected---------\")\n",
    "        print(text1, f\"score: {logit1}\")\n",
    "\n",
    "        return text1\n",
    "    else:\n",
    "        print(\"selected---------\")\n",
    "        print(text2,  f\"score: {logit2}\")\n",
    "\n",
    "        return text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally,  evaluate the performance of a model by using a pairwise comparison approach over a subset of the data set. It begins by defining N, the number of samples to evaluate, and initializes a counter `correct_selections` to keep track of how many times the model correctly identifies the preferred response. The code then iterates over the first N pairs of chosen and rejected responses from the training data set (`train_str['chosen']` and `train_str['rejected']`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of samples to evaluate\n",
    "N = 10\n",
    "\n",
    "# Initialize a counter for correct selections\n",
    "correct_selections = 0\n",
    "\n",
    "# Iterate over the first N pairs of chosen and rejected responses\n",
    "for chosen, rejected in zip(train_str['chosen'][0:N], train_str['rejected'][0:N]):\n",
    "    # Print the chosen response for reference\n",
    "    print(\"Chosen Response:\\n\", chosen)\n",
    "    \n",
    "    # Use the compare_texts function to determine which response is better\n",
    "    selected_text = compare_texts(chosen, rejected)\n",
    "    \n",
    "    # Check if the selected text is the chosen response\n",
    "    if selected_text == chosen:\n",
    "        correct_selections += 1\n",
    "\n",
    "# Calculate the accuracy as the ratio of correct selections to the total number of samples\n",
    "accuracy = correct_selections / N\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "#### Evaluate model's preference accuracy on a different subset of data\n",
    "\n",
    "1. Define a new variable `K` to set the number of samples for evaluation from a different subset of the data.\n",
    "2. Initialize a counter to track the number of correct selections made by the model.\n",
    "3. Iterate over the `K` pairs of chosen and rejected responses from a different subset of the data set (for example, from the middle of the data set).\n",
    "4. For each pair, use the `compare_texts` function to determine which response is better.\n",
    "5. Count the number of times the model correctly identifies the chosen response.\n",
    "6. Calculate and print the accuracy of the model's preferences on this different subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "# Define the number of samples to evaluate from a different subset\n",
    "K = 50\n",
    "\n",
    "# Initialize a counter for correct selections\n",
    "correct_selections = 0\n",
    "\n",
    "# Determine the starting index for the different subset (e.g., middle of the dataset)\n",
    "start_index = len(train_str['chosen']) // 2\n",
    "\n",
    "# Iterate over K pairs of chosen and rejected responses from the different subset\n",
    "for chosen, rejected in zip(train_str['chosen'][start_index:start_index + K], train_str['rejected'][start_index:start_index + K]):\n",
    "    # Use the compare_texts function to determine which response is better\n",
    "    selected_text = compare_texts(chosen, rejected)\n",
    "    \n",
    "    # Check if the selected text is the chosen response\n",
    "    if selected_text == chosen:\n",
    "        correct_selections += 1\n",
    "\n",
    "# Calculate the accuracy as the ratio of correct selections to the total number of samples\n",
    "accuracy = correct_selections / K\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy on different subset:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Define the number of samples to evaluate from a different subset\n",
    "K = 50\n",
    "\n",
    "# Initialize a counter for correct selections\n",
    "correct_selections = 0\n",
    "\n",
    "# Determine the starting index for the different subset (e.g., middle of the dataset)\n",
    "start_index = len(train_str['chosen']) // 2\n",
    "\n",
    "# Iterate over K pairs of chosen and rejected responses from the different subset\n",
    "for chosen, rejected in zip(train_str['chosen'][start_index:start_index + K], train_str['rejected'][start_index:start_index + K]):\n",
    "    # Use the compare_texts function to determine which response is better\n",
    "    selected_text = compare_texts(chosen, rejected)\n",
    "    \n",
    "    # Check if the selected text is the chosen response\n",
    "    if selected_text == chosen:\n",
    "        correct_selections += 1\n",
    "\n",
    "# Calculate the accuracy as the ratio of correct selections to the total number of samples\n",
    "accuracy = correct_selections / K\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy on different subset:\", accuracy)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo) has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "[Ashutosh Sagar](https://www.linkedin.com/in/ashutoshsagar/) is completing his MS in CS from Dalhousie University. He has previous experience working with Natural Language Processing and as a Data Scientist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Contributors\n",
    "\n",
    "[Hailey Quach](https://author.skills.network/instructors/hailey_quach) is a Data Scientist at IBM. She's completing her Bsc, Honors in Computer Science at Concordia University, Montreal.\n",
    "\n",
    "[Wojciech \"Victor\" Fulmyk](https://www.linkedin.com/in/wfulmyk) is a Data Scientist at IBM and a PhD student in Economics at the University of Calgary. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "1adfc1049bb96da41db2a3a0e921ab3adca6669439d9ccb75cfcf8942f433f77"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
